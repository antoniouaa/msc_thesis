In this section, we provide a brief history of neural networks in the context of finance and time series prediction, as well as a more detailed description of the architecture used in the paper.

Neural networks are systems largely belonging to the study of Machine Learning, typically associated with solving computational problems that other models and algorithms struggle to. Loosely based on biological neural networks, like the brain, artificial neural networks are collections of neuron-like nodes and links that connect them. Their resemblance to the brain in terms of architecture is part of what allowed neural networks to grow in computing power and popularity over the past decades, from their emergence in the 1940s\cite{nervenets}\cite{nervous_activity_warrenpitts}.

\section{Neurons}
Neurons are the artificial equivalent of a biological neuron and are the fundamental components of a neural network. Neurons perform three tasks, receive input from other neurons, apply an activation function to the input and output a value to other neurons. Neurons in the network are connected and each of these connections carries a signal and has certain weight attached to it, which affects the signal carried\cite{machine-learning-dict}\cite{ai_book}. Graphically, a neuron could be represented in figure \ref{tab:neuron}.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        init/.style={
        draw,
        circle,
        inner sep=2pt,
        font=\Huge,
        join = by -latex
        },
        squa/.style={
        draw,
        inner sep=2pt,
        font=\Large,
        join = by -latex
        },
        start chain=2,node distance=13mm
        ]
        \node[on chain=2] 
        (x2) {$x_2$};
        \node[on chain=2,join=by o-latex] 
        {$w_2$};
        \node[on chain=2,init] (sigma) 
        {$\displaystyle\Sigma$};
        \node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activation \\ function}}]   
        {$f$};
        \node[on chain=2,label=above:Output,join=by -latex] 
        {$y$};
        \begin{scope}[start chain=1]
        \node[on chain=1] at (0,1.5cm) 
        (x1) {$x_1$};
        \node[on chain=1,join=by o-latex] 
        (w1) {$w_1$};
        \end{scope}
        \begin{scope}[start chain=3]
        \node[on chain=3] at (0,-1.5cm) 
        (x3) {$x_3$};
        \node[on chain=3,label=below:Weights,join=by o-latex] 
        (w3) {$w_3$};
        \end{scope}
        \node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};
        
        \draw[-latex] (w1) -- (sigma);
        \draw[-latex] (w3) -- (sigma);
        \draw[o-latex] (b) -- (sigma);
        
        \draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
    \end{tikzpicture}
    \caption{Graphical representation of a Neuron}
    \label{tab:neuron}
\end{figure}

\section{Activation Functions}
\citeauthor{machine-learning-dict} (\citeyear{machine-learning-dict}) defines the activation function of a neural network as the function that governs the output behaviour of a neuron, given a set of input values. There are several types of activation functions, the simplest being the step function.

In mathematical terms, the step function, or the unit step function, is defined as

\begin{align}
    \centering
    H(x) := \begin{cases}
        0, & \text{for } x < 0 \\
        1, & \text{for } x \geq 0
    \end{cases}
\end{align}

where $H(x)$ is the Heaviside step function\cite{step_function}, and at value 0, an output of $H(0) = 1$ is selected and passed down to the next layer of neurons.

\section{Backpropagation}
The backpropagation algorithm is a key feature of neural networks as it allowed for fast and efficient training of multi-layered networks by distributing error values back through the layers of the network, adjusting the weights in the connections between neurons respectively\cite{backprop}. In more detail, backpropagation functions compute the gradient $\nabla_{x}f(\mathbf{x}, \mathbf{y})$ numerically in a simple and inexpensive procedure\cite{rumelhart_backprop}.

\newcommand{\R}{\mathbb{R}}

The chain rule is a way to calculate the derivate of functions by decomposing a function to other functions whose derivative is known. Let $x$ be a real number and $f$ and $g$ be functions that map from $\R$ to $\R$. By generalising the chain rule of calculus

\begin{align}
    \frac{dz}{dx} &= \frac{dz}{dy}\frac{dy}{dx}
\end{align}

beyond the scalar case into vector notation, such that 

\begin{align}
    \nabla_{x}z = (\frac{\delta y}{\delta x})^T \nabla_{y}z
\end{align}

where $\frac{\delta y}{\delta x}$ is a Jacobian matrix of $g$. The backpropagation algorithm is essentially a fast computation of this Jacobian gradient for each connection and node in the network\cite{goodfellow_backprop}.

\section{ARIMA}
The prices of stocks can be modelled as non-linear time series, which have been at the centre of attention in the finance world since the 1970s with George Box and Gwilym Jenkins popularised their Box-Jenkins method for finding the best-fit of a time series model\cite{box_jenkins}.

\section{Testing some math}
Here are two equations:





And here is some text with some nice inline math, $(x, y)$ wow $\gamma$ so cool $\rho$.


\section{Testing citations}
This is Fama\cite{fama_efficient_market} and this is Goodfellow.
This is another GAN citation.